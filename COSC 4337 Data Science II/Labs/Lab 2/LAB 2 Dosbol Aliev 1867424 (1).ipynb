{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31cf2b13",
   "metadata": {},
   "source": [
    "# <center>Lab 2 - Basic Neural Network</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb91209",
   "metadata": {},
   "source": [
    "# <center>COSC 4337 - Data Science II Dr. Rizk</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4088908a",
   "metadata": {},
   "source": [
    "# <center>DOSBOL ALEIV </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de81e95",
   "metadata": {},
   "source": [
    "# <center>MyUH 1867424 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc7a5b",
   "metadata": {},
   "source": [
    "#### 1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f34ea3",
   "metadata": {},
   "source": [
    "#### In this lab we will work with a basic multi-layer perceptron neural network in order to create an xor logic gate.\n",
    "We are going to create a XOR logic gate by combineing the outputs of an \"Nand\" gate (inverse \"and\" gate) and an \"or\" gate and finally combine the results in an \"and\" gate. We will do this through use of a multi-layer perceptron by first taking the values for the \"Nand\" and \"or\" logic gates and running them through perceptrons then aggregating them and applying an activation function to either get 1 or 0.\n",
    "Here is an image of what I am describing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e131385",
   "metadata": {},
   "source": [
    "Futhermore this comes from: https://towardsdatascience.com/how-neural-networks-solve-the-xor-problem-59763136bdd7\n",
    "It is highly recommend to use this above article as a reference.\n",
    "If you have any questions about logic gates you can check out this wiki article what we are concerned with is truth tables on the right of the graph when you scroll down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7256e0d6",
   "metadata": {},
   "source": [
    "#### The objective of this lab is to get the neural network weights + summation function to output the truth table of a XOR gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2843e8bf",
   "metadata": {},
   "source": [
    "First we need to start by importing some packages and defining the values of our weights and biases. For now simply use the values below the bias and learning rate (lr). They can be change to your liking though I recommend leaving them as is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8274378",
   "metadata": {},
   "source": [
    "Next we want to write a function to train the weights of the 1st layer. Since we are considering the \"Nand\" and \"or\" we want to generalize this functions to train them for both.\n",
    "\n",
    "Now since we want to translate this into a perceptron what we will do is take the inputs (input1 and input2) and multiply them by the weights. We also want to multiply the last weight by the bias and add that in as well.\n",
    "Finally we want to compare the output we get and compare it to the expected output(output vs outputP) this will give us the error\n",
    "After we have the error we want to change the weights in order to get a more accurate result. This is known as back propogation . In order to change the weights we want to multiply the error by the relavent input (input1 for w[0]) and multiply it by the learning rate. This will allow for the the weights to move towards better values.\n",
    "You can play around with the learning rate here to see what happens.\n",
    "\n",
    "Fill in the ??? with the relavent information as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de28118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.40483999156619355, 0.7135839085738344, 0.06207465636397491]\n"
     ]
    }
   ],
   "source": [
    "import numpy, random , os \n",
    "lr = .1\n",
    "bias = 1\n",
    "bias2=1\n",
    "weights = [random.random(),random.random(),random.random()]\n",
    "weights2 = [random.random(),random.random(),random.random()]\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f032e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perceptron(input1,input2,output,w,b):\n",
    "    outputP =input1*w[0] +input2*w[1]+b*w[2]\n",
    "    if outputP >0:\n",
    "        outputP =1 \n",
    "    else:\n",
    "        outputP = 0\n",
    "    error =output -outputP\n",
    "    print(outputP)\n",
    "    print(error)\n",
    "    w[0]+= error* lr * input1\n",
    "    w[1]+= error* lr * input2\n",
    "    w[2]+= error* lr * bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e0cbe",
   "metadata": {},
   "source": [
    "Here we are training the weights. Remember we want to combine the \"Nand\" and \"or\" logic gates so here we need to input the truth tables for each gate and use those values to train the perceptrons to emulate the truth tables. 2 logic tables = 2 perceptrons = 2 sets of weights and biases\n",
    "\n",
    "An epoch is running through all inputs in a neural network once. In this case by default we set it to 50 so the inputs will all run through the 50 times. So 8 inputs 50 epochs = 400 instances of back propogation so 200 per perceptron. Here we consider a batch size of 1. Which is the size of the number of inputs that go through the network before an instance of back propogation. You can check this out for more information of the hyperparameters I have just discuess.\n",
    "\n",
    "Fill in the ??? with the relavent information as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "540334d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#training weights\n",
    "epochs = 50\n",
    "for i in range(epochs):\n",
    "    Perceptron(1,1,1,weights,bias)\n",
    "    Perceptron(1,0,1,weights,bias)\n",
    "    Perceptron(0,1,1,weights,bias)\n",
    "    Perceptron(0,0,0,weights,bias)\n",
    "    \n",
    "    Perceptron(1,1,0,weights2,bias2)\n",
    "    Perceptron(1,0,1,weights2,bias2)\n",
    "    Perceptron(0,1,1,weights2,bias2)\n",
    "    Perceptron(0,0,1,weights2,bias2)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf14fef",
   "metadata": {},
   "source": [
    "Finally now that we have the weights trained we can take the outputs of both perceptrons and aggregate them. *** Insert the equations for the relavent perceptrons then aggregate them. There are multiple ways to do this but the simpliest way is to use an operator.\n",
    "\n",
    "Fill in the ??? with the relavent information as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06055259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summation(input1,input2,w1,b1,w2,b2):\n",
    "    \n",
    "    O1 = input1 * w1[0]+ input1 * w1[1] + b1* w1[2] #Nand gate\n",
    "    O2 = input2 * w2[0]+ input2 * w2[1] + b2* w2[2] #or gate\n",
    "    \n",
    "\n",
    "    outputP = (O1 *O2)# this should be an operator play around and find out which one it should be.\n",
    "    # this is the and gate\n",
    "    if  outputP > 0:\n",
    "        outputP =1 \n",
    "    else:\n",
    "        outputP = 0\n",
    "    \n",
    "    return outputP\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80fa93",
   "metadata": {},
   "source": [
    "Here we can test whether your training of weights works all you need to do is insert a sigmoid activation function on the the out variable.\n",
    "You can check out the sigmoid function here.\n",
    "If you are interested in checking out other types of activation functions you can look here.\n",
    "\n",
    "Finally you can see from this sample the expected output of the feed forward neural network. Don't worry too much if you have different numbers for the activation function what matters is getting the right output (the value of out).\n",
    "\n",
    "Fill in the ??? with the relavent information as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1838ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 or 1 is:  0  :  0.5\n",
      "1 or 0 is:  1  :  0.7310585786300049\n",
      "0 or 1 is:  1  :  0.7310585786300049\n",
      "0 or 0 is:  0  :  0.5\n"
     ]
    }
   ],
   "source": [
    "val = [1,0]\n",
    "for x in val:\n",
    "    for y in val:\n",
    "        out = summation(x,y,weights,bias,weights2,bias2)\n",
    "        print(x,\"or\",y,\"is: \",out,\" : \",(1.0 / (1.0 + numpy.exp(-out))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
